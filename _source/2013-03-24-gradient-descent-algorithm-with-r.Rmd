---
layout: post
title: "Demonstration of the Gradient Descent Algorithm"
author: [yulijia]
categories: [Animation, Computational Statistics]
tags: [gradient descent algorithm]
reviewer: [yihui]
animation: true
---
{% include JB/setup %}

In the [**animation** package](http://yihui.name/animation), there is a function named
`grad.desc()`. It provides a visual illustration for the process of minimizing a real-valued
function through the [Gradient Descent Algorithm](http://en.wikipedia.org/wiki/Gradient_descent).
The two examples below show you how to use the `grad.desc()` function.

## A simple function

The default objective function in `grad.desc()` is $$f(x,y)=x{^2}+2y{^2}$$. The arrows will take
you to the minima step by step:

```{r grad-desc-right, fig.show='animate', interval=.2}
library(animation)
par(mar = c(4, 4, 2, .1))
grad.desc()
```

## When the algorithm fails

This example shows how the gradient descent algorithm will fail with a too large step length.

To find a local minimum of a bivariate objective function:

$$z=\sin(\frac{1}{2}x{^2}-\frac{1}{4}y{^4}+3)\cos(2x+1-e{^y})$$

```{r grad-desc-wrong, fig.show='animate', interval=.2, fig.height=8, fig.width=8}
ani.options(nmax = 70)
par(mar = c(4, 4, 2, .1))
f2 = function(x, y) sin(1/2 * x^2 - 1/4 * y^2 + 3) * cos(2 * x + 1 - exp(y))
grad.desc(f2, c(-2, -2, 2, 2), c(-1, 0.5), gamma = 0.3, tol = 1e-04)
```

Apparently the arrows get lost eventually. You can replace `gamma=0.3` with a smaller value and
retry the function.
